
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>LiveNVS</title>

    <meta name="description" content="LiveNVS is a system for real-time neural view synthesis on live RGB-D streams. It achieves photo-realism for unknown scenes during capturing, allowing instant exploration of the virtual scene." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://lorafib.github.io/images/livenvs_teaser.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="750">
    <meta property="og:image:height" content="500">
    <meta property="og:type" content="website" />
    <meta property="og:title" content="LiveNVS: Neural View Synthesis on Live RGB-D Streams" />
    <meta property="og:description" content="LiveNVS is a system for real-time neural view synthesis on live RGB-D streams. It achieves photo-realism for unknown scenes during capturing, allowing instant exploration of the virtual scene." />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://lorafib.github.io/livenvs/" />
    <meta name="twitter:title" content="LiveNVS: Neural View Synthesis on Live RGB-D Streams" />
    <meta name="twitter:description" content="LiveNVS is a system for real-time neural view synthesis on live RGB-D streams. It achieves photo-realism for unknown scenes during capturing, allowing instant exploration of the virtual scene." />
    <meta name="twitter:image" content="https://lorafib.github.io/images/livenvs_teaser.jpg" />


<!--
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
//-->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>

    <link rel="stylesheet" href="css/dics.min.css">
    <script src="js/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
</head>

<body>
    <div class="container" id="main">
        <a href="..">Back to my projects</a>
        <br><br>
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>LiveNVS</b>: Neural View Synthesis on Live RGB-D Streams
                </br>
                <small>
                    Siggraph Asia 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a style="text-decoration:none" href="https://lorafib.github.io">
                            Laura Fink
                        </a>

                        <br>FAU Erlangen-Nürnberg
                        <br>Fraunhofer IIS
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/darius-rueckert/">
                            Darius Rückert
                        </a>
                        <br>FAU Erlangen-Nürnberg
                        <br>Voxray GmbH
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://lfranke.github.io">
                            Linus Franke
                        </a>
                        <br>FAU Erlangen-Nürnberg<br> &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none">
                            Joachim Keinert
                        </a>
                        <br>Fraunhofer IIS<br> &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/marc-stamminger/">
                            Marc Stamminger
                        </a>
                        <br>FAU Erlangen-Nürnberg
                        <br>  &zwnj;
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2311.16668">
                            <image src="media/paper_icon.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/aMbE5WAgD2k">
                            <image src="media/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Fraunhofer-IIS/livenvs">
                            <image src="media/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="media/teaser_animated_cropped.mp4" type="video/mp4" />
                </video>
						</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Existing real-time RGB-D reconstruction approaches, like Kinect Fusion, lack real-time photo-realistic visualization.
                    This is due to noisy, oversmoothed or incomplete geometry and blurry textures which are fused from imperfect depth maps and camera poses.
                    Recent neural rendering methods can overcome many of such artifacts but are mostly optimized for offline usage, hindering the integration into a live reconstruction pipeline.
                </p>
                <p class="text-justify">
                    In this paper, we present LiveNVS, a system that allows for neural novel view synthesis on a live RGB-D input stream with very low latency and real-time rendering.
                    Based on the RGB-D input stream, novel views are rendered by projecting neural features into the target view via a densely fused depth map and aggregating the features in image-space to a target feature map.
                    A generalizable neural network then translates the target feature map into a high-quality RGB image.
                    LiveNVS achieves state-of-the-art neural rendering quality of unknown scenes during capturing, allowing users to virtually explore the scene and assess reconstruction quality in real-time.
                                            
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube-nocookie.com/embed/aMbE5WAgD2k" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results: Generalizability
                </h3>
                The LiveNVS pipeline was trained only once on Tanks & Temples and Scannet scenes.
                All results were produced using a single set of weights. We did no scene or dataset specific finetuning.
                It produces high-quality output for indoor and outdoor scenes which are part of self-recorded dataset.
                <br>
                <br>
                <table style="width: 100%; border-collapse: collapse;">
                    <tbody>
                    <tr>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/various_scenes/wohnzimmer5_deblurred_faster.mp4" type="video/mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/various_scenes/supplemental_authors-2.mp4" type="video/mp4" />
                        </video>
                      </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
                          <video id="v0" width="95%" autoplay loop muted controls>
                              <source src="media/various_scenes/erl_sport2_deblurred_faster.mp4" type="video/mp4" />
                          </video>
                        </td>
                        <td style="text-align: center;">
                          <video id="v0" width="95%" autoplay loop muted controls>
                              <source src="media/various_scenes/bauer_shop2_faster-2.mp4" type="video/mp4" />
                          </video>
                        </td>
                      </tr>
                    <tr>
                      <td style="text-align: center;">Indoor</td>
                      <td style="text-align: center;">Outdoor</td>
                    </tr>
                  </tbody></table>     
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results: Interactive Scene Exploration
                </h3>
                We can explore the scene while the dataset is still growing or after the capturing is done.
                
                <br>
                <br>
                <table style="width: 100%; border-collapse: collapse;">
                    <tbody>
                    <tr>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/interactive/2023-11-23 16-46-49.mkv-2.mp4" type="video/mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/interactive/2023-11-23_16-23-36.mkv-2.mp4" type="video/mp4" />
                        </video>
                      </td>
                    </tr>
                  </tbody></table>     
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results: Instant Respone to Pose Updates
                </h3>
                The system will immediately reflect if the SLAM module refines camera poses.
                
                <br>
                <br>
                <table style="width: 100%; border-collapse: collapse;">
                    <tbody>
                    <tr>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/loop/slides_loop_closure.mp4" type="video/mp4" />
                        </video>
                      </td>
                      <td style="text-align: center;">
                        <video id="v0" width="95%" autoplay loop muted controls>
                            <source src="media/loop/slides_loop_closure_toggle.mp4" type="video/mp4" />
                        </video>
                      </td>
                    </tr>
                  </tbody></table>     
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results: Comparison of Live Novel View Synthesis
                </h3>
                We compare our method against INGP [Müller 2022] and FWD [Cao 2023], simulating the novel view synthesis quality and performance live during capturing.
                The values on the left indicate LPIPS (lower is better) and render time in seconds on the right.
                
                <br>
                <br>
                <table style="width: 100%; border-collapse: collapse;">
                    <tbody>
                    <tr>
                      <td style="text-align: center;">
                        <img src="media/comparison/erl_wall_figure.png" width="95%">
                      </td>
                      <td style="text-align: center;">
                        <img src="media/comparison/wohnzimmer5_comparison.png" width="95%">
                      </td>
                      <td>

                      </td>
                    </tr>
                  </tbody></table>     
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-12 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" readonly>
@article{fink2023livenvs,
    title={LiveNVS: Neural View Synthesis on Live RGB-D Streams},
    author={Laura Fink and Darius R{\"u}ckert and Linus Franke and Joachim Keinert and Marc Stamminger},
    booktitle = {ACM SIGGRAPH Asia 2023 Conference Proceedings},
    year = {2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>

                We would like to thank all members of the Visual Computing Lab Erlangen for their support and fruitful discussions.
                Specifically, we appreciate Mathias Harrer's contribution to the evaluation and Dominik Penk's help regarding the dataset preparation.
                We also thank Ashutosh Mishra for his insights about prior arts.
                <br>
                The authors gratefully acknowledge the scientific support and HPC resources provided by the National High Performance Computing Center  of the Friedrich-Alexander-Universität Erlangen-Nürnberg (NHR@FAU) under the project b162dc. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) – 440719683.
                Linus Franke was supported by the Bavarian Research Foundation (Bay. Forschungsstiftung) AZ-1422-20.
                Joachim Keinert was supported by the Free State of Bavaria in the DSAI project.
                
                <br>
                <br>
                

                <p class="text-justify">
                    The website template was adapted from  <a href="https://lfranke.github.io/vet">VET</a>, who borrowed from <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>, who borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                    Image sliders are from <a href="https://bakedsdf.github.io/">BakedSDF</a>.
                </p>
            </div>
        </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                References
            </h3>
    <div class="content has-text-justified">
        <p>
            [Müller 2022] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. 2022a. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG) 41, 4 (2022), 1–15.
        </p><p>    
            [Cao  2023] Ang Cao, Chris Rockwell, and Justin Johnson. 2022. FWD: Real-Time Novel View Synthesis With Forward Warping and Depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15713–15724.
        </p>
      </div>
    </div>

</body>
</html>
